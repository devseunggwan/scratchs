{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import col, pandas_udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"chaper-5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(s):\n",
    "    return s ** 3\n",
    "\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   id\n",
    "               , cubed(id) AS id_cubed\n",
    "          FROM   udf_test\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   s\n",
    "          FROM   test1\n",
    "         WHERE   s IS NOT NULL\n",
    "           AND   strlen(s) > 1\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1, 9)\n",
    "df.select(\"id\", cubed_udf(col(\"id\")).alias(\"cubed_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "* PostgreSQL\n",
    "* MySQL\n",
    "* Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgresql\n",
    "\n",
    "# read1\n",
    "jdbc_df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# read2\n",
    "jdbc_df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA]:[TABLENAME]\", properties={\"user\": \"[USERNAME]\", \"password\": [\"PASSWORD\"]})\n",
    ")\n",
    "\n",
    "# write1\n",
    "(\n",
    "    jdbc_df1\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "# write2\n",
    "(\n",
    "    jdbc_df2\n",
    "    .write\n",
    "    .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA]:[TABLENAME]\", properties={\"user\": \"[USERNAME]\", \"password\": [\"PASSWORD\"]})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql\n",
    "\n",
    "# read\n",
    "jdbc_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "    .option(\"driver\", \"com.mysql.jdbcDriver\")\n",
    "    .option(\"dbtable\", \"[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# write\n",
    "(\n",
    "    jdbc_df\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "    .option(\"driver\", \"com.mysql.jdbcDriver\")\n",
    "    .option(\"dbtable\", \"[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://docs.snowflake.com/ko/user-guide/spark-connector-use\n",
    "\n",
    "sc = SparkContext(\"local\", \"chaper-5\")\n",
    "spark = SQLContext(sc)\n",
    "spark_conf = SparkConf().setMaster(\"local\").setAppName(\"chaper-5-snowflake-app\")\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"[AWS_KEY]\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"[AWS_SECRET_KEY]\")\n",
    "\n",
    "# use password\n",
    "sf_option = {\n",
    "    \"sfURL\": \"<account-identifier>.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"<user-name>\",\n",
    "    \"sfPassword\": \"<password>\",\n",
    "    \"sfDatabase\": \"<database>\",\n",
    "    \"sfSchema\": \"<schema>\",\n",
    "    \"sfWarehouse\": \"<warehouse>\"\n",
    "}\n",
    "\n",
    "# use oauth token (recommand)\n",
    "sf_option = {\n",
    "    \"sfURL\": \"<account-identifier>.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"<user-name>\",\n",
    "    \"sfAuthenticator\": \"oauth\",\n",
    "    \"sfToken\": \"<external-oauth-access-token>\",\n",
    "    \"sfDatabase\": \"<database>\",\n",
    "    \"sfSchema\": \"<schema>\",\n",
    "    \"sfWarehouse\": \"<warehouse>\"\n",
    "}\n",
    "\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "QUERY = \"\"\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(SNOWFLAKE_SOURCE_NAME)\n",
    "    .option(**sf_option)\n",
    "    .option(\"query\", QUERY)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Build-In Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|array_distinct(array(1, 2, 3, NULL, 3))|\n",
      "+---------------------------------------+\n",
      "|                        [1, 2, 3, NULL]|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_distinct(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|array_intersect(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-----------------------------------------------+\n",
      "|                                         [1, 3]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_intersect(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|array_union(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-------------------------------------------+\n",
      "|                               [1, 2, 3, 5]|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_union(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_except(array(1, 2, 3), array(1, 3, 5))|\n",
      "+--------------------------------------------+\n",
      "|                                         [2]|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_except(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_join(array(hello, world),  )|\n",
      "+----------------------------------+\n",
      "|                       hello world|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_join(array('hello', 'world'), ' ')\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_max(array(1, 2, 3, NULL, 3))|\n",
      "+----------------------------------+\n",
      "|                                 3|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_max(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_min(array(1, 2, 3, NULL, 3))|\n",
      "+----------------------------------+\n",
      "|                                 1|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_min(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|array_position(array(3, 2, 1), 1)|\n",
      "+---------------------------------+\n",
      "|                                3|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_position(array(3, 2, 1), 1)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|array_remove(array(1, 2, 3, NULL, 3), 3)|\n",
      "+----------------------------------------+\n",
      "|                            [1, 2, NULL]|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_remove(array(1, 2, 3, null, 3), 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|arrays_overlap(array(1, 2, 3), array(3, 4, 5))|\n",
      "+----------------------------------------------+\n",
      "|                                          true|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|array_sort(array(b, d, NULL, c, a), lambdafunction((IF(((namedlambdavariable() IS NULL) AND (namedlambdavariable() IS NULL)), 0, (IF((namedlambdavariable() IS NULL), 1, (IF((namedlambdavariable() IS NULL), -1, (IF((namedlambdavariable() < namedlambdavariable()), -1, (IF((namedlambdavariable() > namedlambdavariable()), 1, 0)))))))))), namedlambdavariable(), namedlambdavariable()))|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                                                                                                            [a, b, c, d, NULL]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_sort(array('b', 'd', null, 'c', 'a'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|concat(array(1, 2, 3), array(3, 4, 5))|\n",
      "+--------------------------------------+\n",
      "|                    [1, 2, 3, 3, 4, 5]|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT concat(array(1, 2, 3), array(3, 4, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|flatten(array(array(1, 2, 3), array(3, 4, 5)))|\n",
      "+----------------------------------------------+\n",
      "|                            [1, 2, 3, 3, 4, 5]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT flatten(array(array(1, 2, 3), array(3, 4, 5)))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|array_repeat(123, 3)|\n",
      "+--------------------+\n",
      "|     [123, 123, 123]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_repeat('123', 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|reverse(array(2, 1, 4, 3))|\n",
      "+--------------------------+\n",
      "|              [3, 4, 1, 2]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT reverse(array(2, 1, 4, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| sequence(1, 5)|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(1, 5);\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| sequence(5, 1)|\n",
      "+---------------+\n",
      "|[5, 4, 3, 2, 1]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(5, 1);\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|sequence(to_date(2018-01-01), to_date(2018-03-01), INTERVAL '1' MONTH)|\n",
      "+----------------------------------------------------------------------+\n",
      "|                                                  [2018-01-01, 2018...|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|shuffle(array(1, 2, NULL, 3))|\n",
      "+-----------------------------+\n",
      "|              [2, 1, 3, NULL]|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT shuffle(array(1, 2, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|slice(array(1, 2, 3, 4), -2, 2)|\n",
      "+-------------------------------+\n",
      "|                         [3, 4]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT slice(array(1, 2, 3, 4), -2, 2)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|arrays_zip(array(1, 2, 3, 4), array(5, 6, 7, 8), array(9, 10, 11, 12))|\n",
      "+----------------------------------------------------------------------+\n",
      "|                                                  [{1, 5, 9}, {2, 6...|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT arrays_zip(array(1, 2, 3, 4), array(5, 6, 7, 8), array(9, 10, 11, 12))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|element_at(array(3, 1, 2), 3)|\n",
      "+-----------------------------+\n",
      "|                            2|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT element_at(array(3, 1, 2), 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|cardinality(array(b, d, c, a))|\n",
      "+------------------------------+\n",
      "|                             4|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT cardinality(array('b', 'd', 'c', 'a'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|map_from_arrays(array(1.0, 3.0), array(2, 4))|\n",
      "+---------------------------------------------+\n",
      "|                         {1.0 -> 2, 3.0 -> 4}|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|map_from_entries(array(struct(1, a), struct(2, b)))|\n",
      "+---------------------------------------------------+\n",
      "|                                   {1 -> a, 2 -> b}|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|map_concat(map(1, a, 2, b), map(3, c, 4, d))|\n",
      "+--------------------------------------------+\n",
      "|                        {1 -> a, 2 -> b, ...|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c', 4, 'd'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|element_at(map(1, a, 2, b), 2)|\n",
      "+------------------------------+\n",
      "|                             b|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT element_at(map(1, 'a', 2, 'b'), 2)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|cardinality(map(1, a, 2, b))|\n",
      "+----------------------------+\n",
      "|                           2|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT cardinality(map(1, 'a', 2, 'b'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark High-Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
