{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, pandas_udf, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/06 00:22:14 WARN Utils: Your hostname, codespaces-10f702 resolves to a loopback address: 127.0.0.1; using 10.0.0.198 instead (on interface eth0)\n",
      "25/02/06 00:22:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/06 00:22:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"chaper-5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(s):\n",
    "    return s ** 3\n",
    "\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   id\n",
    "               , cubed(id) AS id_cubed\n",
    "          FROM   udf_test\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   s\n",
    "          FROM   test1\n",
    "         WHERE   s IS NOT NULL\n",
    "           AND   strlen(s) > 1\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1, 9)\n",
    "df.select(\"id\", cubed_udf(col(\"id\")).alias(\"cubed_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "* PostgreSQL\n",
    "* MySQL\n",
    "* Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgresql\n",
    "\n",
    "# read1\n",
    "jdbc_df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# read2\n",
    "jdbc_df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA]:[TABLENAME]\", properties={\"user\": \"[USERNAME]\", \"password\": [\"PASSWORD\"]})\n",
    ")\n",
    "\n",
    "# write1\n",
    "(\n",
    "    jdbc_df1\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "# write2\n",
    "(\n",
    "    jdbc_df2\n",
    "    .write\n",
    "    .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA]:[TABLENAME]\", properties={\"user\": \"[USERNAME]\", \"password\": [\"PASSWORD\"]})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql\n",
    "\n",
    "# read\n",
    "jdbc_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "    .option(\"driver\", \"com.mysql.jdbcDriver\")\n",
    "    .option(\"dbtable\", \"[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# write\n",
    "(\n",
    "    jdbc_df\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "    .option(\"driver\", \"com.mysql.jdbcDriver\")\n",
    "    .option(\"dbtable\", \"[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# https://docs.snowflake.com/ko/user-guide/spark-connector-use\n",
    "\n",
    "sc = SparkContext(\"local\", \"chaper-5\")\n",
    "spark = SQLContext(sc)\n",
    "spark_conf = SparkConf().setMaster(\"local\").setAppName(\"chaper-5-snowflake-app\")\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"[AWS_KEY]\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"[AWS_SECRET_KEY]\")\n",
    "\n",
    "# use password\n",
    "sf_option = {\n",
    "    \"sfURL\": \"<account-identifier>.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"<user-name>\",\n",
    "    \"sfPassword\": \"<password>\",\n",
    "    \"sfDatabase\": \"<database>\",\n",
    "    \"sfSchema\": \"<schema>\",\n",
    "    \"sfWarehouse\": \"<warehouse>\"\n",
    "}\n",
    "\n",
    "# use oauth token (recommand)\n",
    "sf_option = {\n",
    "    \"sfURL\": \"<account-identifier>.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"<user-name>\",\n",
    "    \"sfAuthenticator\": \"oauth\",\n",
    "    \"sfToken\": \"<external-oauth-access-token>\",\n",
    "    \"sfDatabase\": \"<database>\",\n",
    "    \"sfSchema\": \"<schema>\",\n",
    "    \"sfWarehouse\": \"<warehouse>\"\n",
    "}\n",
    "\n",
    "\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "\n",
    "QUERY = \"\"\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(SNOWFLAKE_SOURCE_NAME)\n",
    "    .option(**sf_option)\n",
    "    .option(\"query\", QUERY)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Build-In Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|array_distinct(array(1, 2, 3, NULL, 3))|\n",
      "+---------------------------------------+\n",
      "|                        [1, 2, 3, NULL]|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_distinct(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|array_intersect(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-----------------------------------------------+\n",
      "|                                         [1, 3]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_intersect(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|array_union(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-------------------------------------------+\n",
      "|                               [1, 2, 3, 5]|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_union(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_except(array(1, 2, 3), array(1, 3, 5))|\n",
      "+--------------------------------------------+\n",
      "|                                         [2]|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_except(array(1, 2, 3), array(1, 3, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_join(array(hello, world),  )|\n",
      "+----------------------------------+\n",
      "|                       hello world|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_join(array('hello', 'world'), ' ')\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_max(array(1, 2, 3, NULL, 3))|\n",
      "+----------------------------------+\n",
      "|                                 3|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_max(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|array_min(array(1, 2, 3, NULL, 3))|\n",
      "+----------------------------------+\n",
      "|                                 1|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_min(array(1, 2, 3, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|array_position(array(3, 2, 1), 1)|\n",
      "+---------------------------------+\n",
      "|                                3|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_position(array(3, 2, 1), 1)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|array_remove(array(1, 2, 3, NULL, 3), 3)|\n",
      "+----------------------------------------+\n",
      "|                            [1, 2, NULL]|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_remove(array(1, 2, 3, null, 3), 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|arrays_overlap(array(1, 2, 3), array(3, 4, 5))|\n",
      "+----------------------------------------------+\n",
      "|                                          true|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|array_sort(array(b, d, NULL, c, a), lambdafunction((IF(((namedlambdavariable() IS NULL) AND (namedlambdavariable() IS NULL)), 0, (IF((namedlambdavariable() IS NULL), 1, (IF((namedlambdavariable() IS NULL), -1, (IF((namedlambdavariable() < namedlambdavariable()), -1, (IF((namedlambdavariable() > namedlambdavariable()), 1, 0)))))))))), namedlambdavariable(), namedlambdavariable()))|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                                                                                                            [a, b, c, d, NULL]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_sort(array('b', 'd', null, 'c', 'a'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|concat(array(1, 2, 3), array(3, 4, 5))|\n",
      "+--------------------------------------+\n",
      "|                    [1, 2, 3, 3, 4, 5]|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT concat(array(1, 2, 3), array(3, 4, 5))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|flatten(array(array(1, 2, 3), array(3, 4, 5)))|\n",
      "+----------------------------------------------+\n",
      "|                            [1, 2, 3, 3, 4, 5]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT flatten(array(array(1, 2, 3), array(3, 4, 5)))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|array_repeat(123, 3)|\n",
      "+--------------------+\n",
      "|     [123, 123, 123]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT array_repeat('123', 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|reverse(array(2, 1, 4, 3))|\n",
      "+--------------------------+\n",
      "|              [3, 4, 1, 2]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT reverse(array(2, 1, 4, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| sequence(1, 5)|\n",
      "+---------------+\n",
      "|[1, 2, 3, 4, 5]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(1, 5);\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "| sequence(5, 1)|\n",
      "+---------------+\n",
      "|[5, 4, 3, 2, 1]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(5, 1);\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|sequence(to_date(2018-01-01), to_date(2018-03-01), INTERVAL '1' MONTH)|\n",
      "+----------------------------------------------------------------------+\n",
      "|                                                  [2018-01-01, 2018...|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|shuffle(array(1, 2, NULL, 3))|\n",
      "+-----------------------------+\n",
      "|              [2, 1, 3, NULL]|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT shuffle(array(1, 2, null, 3))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|slice(array(1, 2, 3, 4), -2, 2)|\n",
      "+-------------------------------+\n",
      "|                         [3, 4]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT slice(array(1, 2, 3, 4), -2, 2)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|arrays_zip(array(1, 2, 3, 4), array(5, 6, 7, 8), array(9, 10, 11, 12))|\n",
      "+----------------------------------------------------------------------+\n",
      "|                                                  [{1, 5, 9}, {2, 6...|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT arrays_zip(array(1, 2, 3, 4), array(5, 6, 7, 8), array(9, 10, 11, 12))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|element_at(array(3, 1, 2), 3)|\n",
      "+-----------------------------+\n",
      "|                            2|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT element_at(array(3, 1, 2), 3)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|cardinality(array(b, d, c, a))|\n",
      "+------------------------------+\n",
      "|                             4|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT cardinality(array('b', 'd', 'c', 'a'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|map_from_arrays(array(1.0, 3.0), array(2, 4))|\n",
      "+---------------------------------------------+\n",
      "|                         {1.0 -> 2, 3.0 -> 4}|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|map_from_entries(array(struct(1, a), struct(2, b)))|\n",
      "+---------------------------------------------------+\n",
      "|                                   {1 -> a, 2 -> b}|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|map_concat(map(1, a, 2, b), map(3, c, 4, d))|\n",
      "+--------------------------------------------+\n",
      "|                        {1 -> a, 2 -> b, ...|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c', 4, 'd'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|element_at(map(1, a, 2, b), 2)|\n",
      "+------------------------------+\n",
      "|                             b|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT element_at(map(1, 'a', 2, 'b'), 2)\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|cardinality(map(1, a, 2, b))|\n",
      "+----------------------------+\n",
      "|                           2|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT cardinality(map(1, 'a', 2, 'b'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark High-Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenhelt|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   celsius\n",
    "               , transform(celsius, t -> (((t * 9) div 5) + 32)) AS fahrenhelt\n",
    "          FROM   tC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   celsius\n",
    "               , filter(celsius, t -> t> 38) AS high\n",
    "          FROM   tC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   celsius\n",
    "               , exists(celsius, t -> t = 38) AS threshold\n",
    "          FROM   tC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/06 01:52:49 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-12c46f37-bae7-4bec-9290-c474ab9b612b. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-12c46f37-bae7-4bec-9290-c474ab9b612b\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:166)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: Cannot run program \"rm\": error=0, Failed to exec spawn helper: pid: 49155, signal: 15\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1170)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1089)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:162)\n",
      "\t... 33 more\n",
      "Caused by: java.io.IOException: error=0, Failed to exec spawn helper: pid: 49155, signal: 15\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:295)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:225)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1126)\n",
      "\t... 35 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   celsius\n",
    "               , reduce(\n",
    "                      celsius\n",
    "                    , 0\n",
    "                    , (t, acc) -> t + acc\n",
    "                    , acc -> (acc div size(celsius) * 9 div 5) + 32 \n",
    "                 ) as avgFahrenheit\n",
    "          FROM   tC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_delays_file_path = \"./datasets/flights/departuredelays.csv\"\n",
    "airports_na_file_path = \"./datasets/flights/airport-codes-na.txt\"\n",
    "\n",
    "airports_na = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "    .load(airports_na_file_path)\n",
    ")\n",
    "airports_na.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "departure_delays = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .options(header=\"true\")\n",
    "    .load(trip_delays_file_path)\n",
    ")\n",
    "departure_delays = (\n",
    "    departure_delays\n",
    "    .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "    .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    ")\n",
    "departure_delays.createOrReplaceTempView(\"departure_delays\")\n",
    "\n",
    "foo = (\n",
    "    departure_delays\n",
    "    .filter(expr(\n",
    "            \"\"\"\n",
    "                    origin == 'SEA'\n",
    "                AND destination == 'SFO'\n",
    "                AND date like '01010%'\n",
    "                AND delay > 0\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   *\n",
    "          FROM   airports_na\n",
    "         LIMIT   10\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   *\n",
    "          FROM   departure_delays\n",
    "         LIMIT   10\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bar = departure_delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"foo\")\n",
    "\n",
    "bar.filter(expr(\n",
    "    \"\"\"\n",
    "        origin == 'SEA'\n",
    "        AND destination == 'SFO'\n",
    "        AND date LIKE '01010%'\n",
    "        AND delay > 0                \n",
    "    \"\"\"\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(\n",
    "    airports_na,\n",
    "    airports_na.IATA == foo.origin\n",
    ").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departure_delays_window\")\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "        CREATE TABLE departure_delays_window AS (\n",
    "            SELECT   origin\n",
    "                   , destination\n",
    "                   , SUM(delay) AS total_delays\n",
    "              FROM   departure_delays\n",
    "             WHERE   1=1\n",
    "               AND   origin      IN ('SEA', 'SFO', 'JFK')\n",
    "               AND   destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "             GROUP   \n",
    "                BY   1, 2\n",
    "        )      \n",
    "    \"\"\"\n",
    ")\n",
    "spark.sql(\"SELECT * FROM departure_delays_window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT   origin\n",
    "               , destination\n",
    "               , SUM(TotalDelays) as total_delays\n",
    "          FROM   departure_delays_window\n",
    "         WHERE   origin = '[ORIGIN]'\n",
    "         GROUP\n",
    "            BY   origin, destination\n",
    "         ORDER\n",
    "            BY   SUM(TotalDelays) DESC\n",
    "         LIMIT   3\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        WITH cte AS (\n",
    "            SELECT   origin\n",
    "                   , destination\n",
    "                   , TotalDelays\n",
    "                   , dense_rank() OVER (PARTITION BY origin ORDER BY TotalDelays DESC) AS rank\n",
    "              FROM   departure_delays_window\n",
    "        )\n",
    "        SELECT   origin\n",
    "               , destination\n",
    "               , TotalDelays\n",
    "               , rank\n",
    "          FROM   cte\n",
    "         WHERE   rank <= 3\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-Time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2 = (\n",
    "    foo.withColumn(\n",
    "        \"status\",\n",
    "        expr(\"CASE WHEN delay <= 10 THEN 'On-Time' ELSE 'Delayed' END\")\n",
    "    )\n",
    ")\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-Time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-Time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        NULL|        NULL|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        NULL|        NULL|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH cte AS (\n",
    "        SELECT   destination\n",
    "               , CAST(SUBSTRING(date, 0, 2) AS INT) AS month\n",
    "               , delay\n",
    "          FROM   departure_delays\n",
    "         WHERE   origin = 'SEA'\n",
    "    )\n",
    "    SELECT   *\n",
    "      FROM   cte\n",
    "     PIVOT   (\n",
    "                CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay,\n",
    "                MAX(delay) AS MaxDelay \n",
    "                FOR month IN (1 JAN, 2 FEB)\n",
    "             )\n",
    "     ORDER   BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
